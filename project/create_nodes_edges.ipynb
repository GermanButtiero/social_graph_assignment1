{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b75536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import stanza\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df = pd.read_csv('disney_names_and_genders.csv', header=0)\n",
    "names_df['Name'] = names_df['Name'].str.lower()\n",
    "names_df['Gender'] = names_df['Gender'].map({'M': 'male', 'F': 'female'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8975ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_path = \"movies/*.txt\"\n",
    "files = glob.glob(movies_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836e29ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize,pos,lemma,ner\")\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945f03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_gpe(tokens, entities):\n",
    "    \"\"\"\n",
    "    tokens: list of words in a sentence\n",
    "    entities: list/set of (entity_text, entity_type)\n",
    "    Returns:\n",
    "        - new token list with multi-word entities joined by '-'\n",
    "        - updated set of entities including merged versions\n",
    "    \"\"\"\n",
    "    tokens = [tok for tok in tokens if tok is not None]\n",
    "    text_str = \" \".join(tokens)\n",
    "    merged_entities = set()\n",
    "\n",
    "    for entity_text, ent_type in entities:\n",
    "        if \" \" in entity_text:  # only merge multi-word entities\n",
    "            merged_text = entity_text.replace(\" \", \"-\")\n",
    "            pattern = r'\\b' + re.escape(entity_text) + r'\\b'\n",
    "            text_str = re.sub(pattern, merged_text, text_str)\n",
    "            merged_entities.add((merged_text, ent_type))\n",
    "        else:\n",
    "            merged_entities.add((entity_text, ent_type))\n",
    "\n",
    "    new_tokens = text_str.split(\" \")\n",
    "    return new_tokens, merged_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f289692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gender(name):\n",
    "    # check if name is not an empty string\n",
    "    if name is None:\n",
    "        return 'Unknown'\n",
    "\n",
    "    try:\n",
    "        name = name.lower().strip()\n",
    "        if name == '':\n",
    "            return 'Unknown'\n",
    "    \n",
    "        # if exact match is found\n",
    "        gender = names_df.loc[names_df['Name'] == name]['Gender'].values[0]\n",
    "\n",
    "    except:\n",
    "        # else try to split name and find parts\n",
    "        genders = []\n",
    "        name_parts = name.lower().split(' ')\n",
    "        for part in name_parts:\n",
    "            if part != '':\n",
    "                # if part of the name is found in any of the rows, get name and gender\n",
    "                gender = names_df[names_df.Name.str.contains(part)]['Gender'].to_list()\n",
    "                genders += gender\n",
    "        # count gender occurrences \n",
    "        gender_counts = Counter(genders)\n",
    "        if len(gender_counts.keys()) == 1:\n",
    "            gender = list(gender_counts.keys())[0]\n",
    "        else:\n",
    "            if len(gender_counts.keys()) > 1:\n",
    "                gender = sorted(gender_counts.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "            else:\n",
    "                # length of unq genders is 0\n",
    "                gender = 'Unknown'\n",
    "    \n",
    "    return gender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d85277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token(token):\n",
    "    if token is None:\n",
    "        return None\n",
    "    # Remove everything except letters, numbers, and basic accented characters\n",
    "    token = re.sub(r\"[^A-Za-zÀ-ÖØ-öø-ÿ0-9]+\", \"\", token)\n",
    "    return token.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_window_words(tokens, target_word, window_size=5):\n",
    "    indices = [i for i, token in enumerate(tokens) if token == target_word]\n",
    "    window_words = set()\n",
    "    for index in indices:\n",
    "        start = max(0, index - window_size)\n",
    "        end = min(len(tokens), index + window_size + 1)\n",
    "        for i in range(start, end):\n",
    "            if i != index:\n",
    "                window_words.add(tokens[i])\n",
    "    return window_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba74521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_person_tokens(tokens, person_names):\n",
    "    \"\"\"\n",
    "    Merge multi-word person names in a token list into single tokens with dash.\n",
    "    tokens: list of token strings\n",
    "    person_names: list or set of full PERSON entity strings\n",
    "    \"\"\"\n",
    "    merged_tokens = tokens.copy()\n",
    "    \n",
    "    for name in person_names:\n",
    "        words = name.split()  # [\"Snow\", \"White\"]\n",
    "        i = 0\n",
    "        while i <= len(merged_tokens) - len(words):\n",
    "            # check if consecutive tokens match the entity\n",
    "            if merged_tokens[i:i+len(words)] == words:\n",
    "                # merge them with a dash\n",
    "                merged_tokens[i:i+len(words)] = [\"-\".join(words)]\n",
    "                i += 1  # skip past merged token\n",
    "            else:\n",
    "                i += 1\n",
    "    return merged_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92fc2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"PLOT:\\s*(.*)\"\n",
    "female_pairs = set()\n",
    "pos_tags = {}\n",
    "male_pairs = set()     \n",
    "mixed_pairs = set()      \n",
    "\n",
    "for file_idx, file_path in enumerate(files):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    match = re.search(pattern, text, re.S)\n",
    "    if not match:\n",
    "        print(f\"file {file_idx} skipped (no PLOT): {file_path}\")\n",
    "        continue\n",
    "\n",
    "    plot = match.group(1).strip()\n",
    "    doc = nlp(plot)\n",
    "    \n",
    "    persons = set()\n",
    "    gpe = set()\n",
    "    names = set()\n",
    "\n",
    "    #we check for entities (PERSON and GPE) in the whole plot\n",
    "    for ent in doc.ents:\n",
    "        if ent.type == \"PERSON\":\n",
    "            pers = ent.text.lower() \n",
    "            names.add(pers) #names of people before concatenation of double names \"snow white\"\n",
    "            gender = get_gender(pers)\n",
    "            concat_pers = \"-\".join(pers.split()) #snow-white\n",
    "            if gender in ['female', 'male']: #to avoid unknowns\n",
    "                if concat_pers not in persons:\n",
    "                    persons.add((concat_pers, ent.type, gender)) \n",
    "                    pos_tags[concat_pers.lower()] = {'pos': ent.type, 'gender': gender}\n",
    "        elif ent.type == \"GPE\":\n",
    "            gpe.add((ent.text, ent.type))\n",
    "\n",
    "    female = [person for person,_,gender in persons if gender=='female']\n",
    "    male = [person for person,_,gender in persons if gender=='male']\n",
    "    \n",
    "    #we go through each sentence and we keep only relevant tokens's lemmas and pos.\n",
    "    for sent in doc.sentences: \n",
    "        sent_tokens = [] #list of relevant tokens to find surrounding words\n",
    "        for token in sent.tokens:\n",
    "            word = token.words[0] #dict with information on the token\n",
    "            text = word.text.lower() #the actual word\n",
    "            pos = word.pos #part-of-speech\n",
    "            lemma = clean_token(word.lemma.lower())\n",
    "            upos = word.upos \n",
    "\n",
    "            if text not in stop_words and text not in string.punctuation and upos not in [\"PART\", \"PUNCT\", \"NUM\", \"DET\"]:\n",
    "                sent_tokens.append(text)\n",
    "                if text not in pos_tags:\n",
    "                    pos_tags[text]= {'pos': pos, 'gender': None}\n",
    "\n",
    "        sent_tokens = merge_person_tokens(sent_tokens, names) #we merge tokens such as \"snow white\" to \"snow-white\" to match persons.\n",
    "        \n",
    "        tokenized_text, merged_entities = fix_gpe(sent_tokens, gpe) #we merge token such as \"new york city\" to new-your-city\n",
    "        for entity, pos in merged_entities:\n",
    "            if entity not in pos_tags:\n",
    "                pos_tags[entity.lower()] = {'pos': pos, 'gender': None}\n",
    "        \n",
    "        #for each person we find the previous and next 5 words\n",
    "        for (person, _, gender) in persons: \n",
    "            surrounding_words = find_window_words(tokenized_text, person,5) \n",
    "            \n",
    "            gender_to_check = female if gender ==\"male\" else male\n",
    "            same_gender = True\n",
    "            for name in gender_to_check:\n",
    "                if name in surrounding_words:\n",
    "                    same_gender = False\n",
    "\n",
    "            if surrounding_words:\n",
    "                surrounding_words.add(person)\n",
    "            #we create combinatios of all the words in the surrounding window\n",
    "            combs = combinations(surrounding_words, 2)\n",
    "            #if the opposite gender of the person is in the surrounding words, we add the pairs to mixed\n",
    "            for pair in combs:\n",
    "                if gender == \"male\":\n",
    "                    if same_gender:\n",
    "                        male_pairs.add(pair)\n",
    "                    else:\n",
    "                        mixed_pairs.add(pair)\n",
    "                if gender == \"female\":\n",
    "                    if same_gender:\n",
    "                        female_pairs.add(pair)\n",
    "                    else:\n",
    "                        mixed_pairs.add(pair)\n",
    "  \n",
    "    print(f'file {file_idx} completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c32afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save files\n",
    "with open(\"male_pairs.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for w1, w2 in male_pairs:\n",
    "        f.write(f\"{w1},{w2}\\n\")\n",
    "\n",
    "with open(\"female_pairs.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for w1, w2 in female_pairs:\n",
    "        f.write(f\"{w1},{w2}\\n\")\n",
    "\n",
    "with open(\"mixed_pairs.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for w1, w2 in mixed_pairs:\n",
    "        f.write(f\"{w1},{w2}\\n\")\n",
    "\n",
    "# Save POS tags \n",
    "with open(\"pos_tags.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(pos_tags, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
