{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4b75536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/germa/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/germa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import stanza\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df = pd.read_csv('files/fdisney_names_and_genders.csv', header=0)\n",
    "names_df['Name'] = names_df['Name'].str.lower()\n",
    "names_df['Gender'] = names_df['Gender'].map({'M': 'male', 'F': 'female'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8975ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_path = \"movies/*.txt\"\n",
    "files = glob.glob(movies_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "836e29ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 17:46:48 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 18.3MB/s]                    \n",
      "2025-12-09 17:46:49 INFO: Downloaded file to /Users/germa/stanza_resources/resources.json\n",
      "2025-12-09 17:46:49 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-12-09 17:46:49 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| pos       | combined_charlm           |\n",
      "| lemma     | combined_nocharlm         |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2025-12-09 17:46:49 INFO: Using device: cpu\n",
      "2025-12-09 17:46:49 INFO: Loading: tokenize\n",
      "2025-12-09 17:46:49 INFO: Loading: mwt\n",
      "2025-12-09 17:46:49 INFO: Loading: pos\n",
      "2025-12-09 17:46:50 INFO: Loading: lemma\n",
      "2025-12-09 17:46:51 INFO: Loading: ner\n",
      "2025-12-09 17:46:52 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize,pos,lemma,ner\")\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "945f03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_gpe(tokens, entities):\n",
    "    \"\"\"\n",
    "    tokens: list of words in a sentence\n",
    "    entities: list/set of (entity_text, entity_type)\n",
    "    Returns:\n",
    "        - new token list with multi-word entities joined by '-'\n",
    "        - updated set of entities including merged versions\n",
    "    \"\"\"\n",
    "    tokens = [tok for tok in tokens if tok is not None]\n",
    "    text_str = \" \".join(tokens)\n",
    "    merged_entities = set()\n",
    "\n",
    "    for entity_text, ent_type in entities:\n",
    "        if \" \" in entity_text:  # only merge multi-word entities\n",
    "            merged_text = entity_text.replace(\" \", \"-\")\n",
    "            pattern = r'\\b' + re.escape(entity_text) + r'\\b'\n",
    "            text_str = re.sub(pattern, merged_text, text_str)\n",
    "            merged_entities.add((merged_text, ent_type))\n",
    "        else:\n",
    "            merged_entities.add((entity_text, ent_type))\n",
    "\n",
    "    new_tokens = text_str.split(\" \")\n",
    "    return new_tokens, merged_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f289692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gender(name):\n",
    "    # check if name is not an empty string\n",
    "    if name is None:\n",
    "        return 'Unknown'\n",
    "\n",
    "    try:\n",
    "        name = name.lower().strip()\n",
    "        if name == '':\n",
    "            return 'Unknown'\n",
    "    \n",
    "        # if exact match is found\n",
    "        gender = names_df.loc[names_df['Name'] == name]['Gender'].values[0]\n",
    "\n",
    "    except:\n",
    "        # else try to split name and find parts\n",
    "        genders = []\n",
    "        name_parts = name.lower().split(' ')\n",
    "        for part in name_parts:\n",
    "            if part != '':\n",
    "                # if part of the name is found in any of the rows, get name and gender\n",
    "                gender = names_df[names_df.Name.str.contains(part)]['Gender'].to_list()\n",
    "                genders += gender\n",
    "        # count gender occurrences \n",
    "        gender_counts = Counter(genders)\n",
    "        if len(gender_counts.keys()) == 1:\n",
    "            gender = list(gender_counts.keys())[0]\n",
    "        else:\n",
    "            if len(gender_counts.keys()) > 1:\n",
    "                gender = sorted(gender_counts.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "            else:\n",
    "                # length of unq genders is 0\n",
    "                gender = 'Unknown'\n",
    "    \n",
    "    return gender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1d85277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token(token):\n",
    "    if token is None:\n",
    "        return None\n",
    "    # Remove everything except letters, numbers, and basic accented characters\n",
    "    token = re.sub(r\"[^A-Za-zÀ-ÖØ-öø-ÿ0-9]+\", \"\", token)\n",
    "    return token.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a55a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_window_words(tokens, target_word, window_size=5):\n",
    "    indices = [i for i, token in enumerate(tokens) if token == target_word]\n",
    "    window_words = set()\n",
    "    for index in indices:\n",
    "        start = max(0, index - window_size)\n",
    "        end = min(len(tokens), index + window_size + 1)\n",
    "        for i in range(start, end):\n",
    "            if i != index:\n",
    "                window_words.add(tokens[i])\n",
    "    return window_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fba74521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_person_tokens(tokens, person_names):\n",
    "    \"\"\"\n",
    "    Merge multi-word person names in a token list into single tokens with dash.\n",
    "    tokens: list of token strings\n",
    "    person_names: list or set of full PERSON entity strings\n",
    "    \"\"\"\n",
    "    merged_tokens = tokens.copy()\n",
    "    \n",
    "    for name in person_names:\n",
    "        words = name.split()  # [\"Snow\", \"White\"]\n",
    "        i = 0\n",
    "        while i <= len(merged_tokens) - len(words):\n",
    "            # check if consecutive tokens match the entity\n",
    "            if merged_tokens[i:i+len(words)] == words:\n",
    "                # merge them with a dash\n",
    "                merged_tokens[i:i+len(words)] = [\"-\".join(words)]\n",
    "                i += 1  # skip past merged token\n",
    "            else:\n",
    "                i += 1\n",
    "    return merged_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f92fc2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 0 completed\n",
      "file 1 completed\n",
      "file 2 completed\n",
      "file 3 completed\n",
      "file 4 completed\n",
      "file 5 completed\n",
      "file 6 completed\n",
      "file 7 completed\n",
      "file 8 completed\n",
      "file 9 completed\n",
      "file 10 completed\n",
      "file 11 completed\n",
      "file 12 completed\n",
      "file 13 completed\n",
      "file 14 completed\n",
      "file 15 completed\n",
      "file 16 completed\n",
      "file 17 completed\n",
      "file 18 completed\n",
      "file 19 completed\n",
      "file 20 completed\n",
      "file 21 completed\n",
      "file 22 completed\n",
      "file 23 completed\n",
      "file 24 completed\n",
      "file 25 completed\n",
      "file 26 completed\n",
      "file 27 completed\n",
      "file 28 completed\n",
      "file 29 completed\n",
      "file 30 completed\n",
      "file 31 completed\n",
      "file 32 completed\n",
      "file 33 completed\n",
      "file 34 completed\n",
      "file 35 completed\n",
      "file 36 completed\n",
      "file 37 completed\n",
      "file 38 completed\n",
      "file 39 completed\n",
      "file 40 completed\n",
      "file 41 completed\n",
      "file 42 completed\n",
      "file 43 completed\n",
      "file 44 completed\n",
      "file 45 completed\n",
      "file 46 completed\n",
      "file 47 completed\n",
      "file 48 completed\n",
      "file 49 completed\n",
      "file 50 completed\n",
      "file 51 completed\n",
      "file 52 completed\n",
      "file 53 completed\n",
      "file 54 completed\n",
      "file 55 completed\n",
      "file 56 completed\n",
      "file 57 completed\n",
      "file 58 completed\n",
      "file 59 completed\n",
      "file 60 completed\n",
      "file 61 completed\n",
      "file 62 completed\n",
      "file 63 completed\n",
      "file 64 completed\n",
      "file 65 completed\n",
      "file 66 completed\n",
      "file 67 completed\n",
      "file 68 completed\n",
      "file 69 completed\n",
      "file 70 completed\n",
      "file 71 completed\n",
      "file 72 completed\n",
      "file 73 completed\n",
      "file 74 completed\n",
      "file 75 completed\n",
      "file 76 completed\n",
      "file 77 completed\n",
      "file 78 completed\n",
      "file 79 completed\n",
      "file 80 completed\n",
      "file 81 completed\n",
      "file 82 completed\n",
      "file 83 completed\n",
      "file 84 completed\n",
      "file 85 completed\n",
      "file 86 completed\n",
      "file 87 completed\n",
      "file 88 completed\n",
      "file 89 completed\n",
      "file 90 completed\n",
      "file 91 completed\n",
      "file 92 completed\n",
      "file 93 completed\n",
      "file 94 completed\n",
      "file 95 completed\n",
      "file 96 completed\n",
      "file 97 completed\n",
      "file 98 completed\n",
      "file 99 completed\n",
      "file 100 completed\n",
      "file 101 completed\n",
      "file 102 completed\n",
      "file 103 completed\n",
      "file 104 completed\n",
      "file 105 completed\n",
      "file 106 completed\n",
      "file 107 completed\n",
      "file 108 completed\n",
      "file 109 completed\n",
      "file 110 completed\n",
      "file 111 completed\n",
      "file 112 completed\n",
      "file 113 completed\n",
      "file 114 completed\n",
      "file 115 completed\n",
      "file 116 completed\n",
      "file 117 completed\n",
      "file 118 completed\n",
      "file 119 completed\n",
      "file 120 completed\n",
      "file 121 completed\n",
      "file 122 completed\n",
      "file 123 completed\n",
      "file 124 completed\n",
      "file 125 completed\n",
      "file 126 completed\n",
      "file 127 completed\n",
      "file 128 completed\n",
      "file 129 completed\n",
      "file 130 completed\n",
      "file 131 completed\n",
      "file 132 completed\n",
      "file 133 completed\n",
      "file 134 completed\n",
      "file 135 completed\n",
      "file 136 completed\n",
      "file 137 completed\n",
      "file 138 completed\n",
      "file 139 completed\n",
      "file 140 completed\n",
      "file 141 completed\n",
      "file 142 completed\n",
      "file 143 completed\n",
      "file 144 completed\n",
      "file 145 completed\n",
      "file 146 completed\n",
      "file 147 completed\n",
      "file 148 completed\n",
      "file 149 completed\n",
      "file 150 completed\n",
      "file 151 completed\n",
      "file 152 completed\n",
      "file 153 completed\n",
      "file 154 completed\n",
      "file 155 completed\n",
      "file 156 completed\n",
      "file 157 completed\n",
      "file 158 completed\n",
      "file 159 completed\n",
      "file 160 completed\n",
      "file 161 completed\n",
      "file 162 completed\n",
      "file 163 completed\n",
      "file 164 completed\n",
      "file 165 completed\n",
      "file 166 completed\n",
      "file 167 completed\n",
      "file 168 completed\n",
      "file 169 completed\n",
      "file 170 completed\n",
      "file 171 completed\n",
      "file 172 completed\n",
      "file 173 completed\n",
      "file 174 completed\n",
      "file 175 completed\n",
      "file 176 completed\n",
      "file 177 completed\n",
      "file 178 completed\n",
      "file 179 completed\n",
      "file 180 completed\n",
      "file 181 completed\n",
      "file 182 completed\n",
      "file 183 completed\n",
      "file 184 completed\n",
      "file 185 completed\n",
      "file 186 completed\n",
      "file 187 completed\n",
      "file 188 completed\n",
      "file 189 completed\n",
      "file 190 completed\n",
      "file 191 completed\n",
      "file 192 completed\n",
      "file 193 completed\n",
      "file 194 completed\n",
      "file 195 completed\n",
      "file 196 completed\n",
      "file 197 completed\n",
      "file 198 completed\n",
      "file 199 completed\n",
      "file 200 completed\n",
      "file 201 completed\n",
      "file 202 completed\n",
      "file 203 completed\n",
      "file 204 completed\n",
      "file 205 completed\n",
      "file 206 completed\n",
      "file 207 completed\n",
      "file 208 completed\n",
      "file 209 completed\n",
      "file 210 completed\n",
      "file 211 completed\n",
      "file 212 completed\n",
      "file 213 completed\n",
      "file 214 completed\n",
      "file 215 completed\n",
      "file 216 completed\n",
      "file 217 completed\n",
      "file 218 completed\n",
      "file 219 completed\n",
      "file 220 completed\n",
      "file 221 completed\n",
      "file 222 completed\n",
      "file 223 completed\n",
      "file 224 completed\n",
      "file 225 completed\n",
      "file 226 completed\n",
      "file 227 completed\n",
      "file 228 completed\n",
      "file 229 completed\n",
      "file 230 completed\n",
      "file 231 completed\n",
      "file 232 completed\n",
      "file 233 completed\n",
      "file 234 completed\n",
      "file 235 completed\n",
      "file 236 completed\n",
      "file 237 completed\n",
      "file 238 completed\n",
      "file 239 completed\n",
      "file 240 completed\n",
      "file 241 completed\n",
      "file 242 completed\n",
      "file 243 completed\n",
      "file 244 completed\n",
      "file 245 completed\n",
      "file 246 completed\n",
      "file 247 completed\n",
      "file 248 completed\n",
      "file 249 completed\n",
      "file 250 completed\n",
      "file 251 completed\n",
      "file 252 completed\n",
      "file 253 completed\n",
      "file 254 completed\n",
      "file 255 completed\n",
      "file 256 completed\n",
      "file 257 completed\n",
      "file 258 completed\n",
      "file 259 completed\n",
      "file 260 completed\n",
      "file 261 completed\n",
      "file 262 completed\n",
      "file 263 completed\n",
      "file 264 completed\n",
      "file 265 completed\n",
      "file 266 completed\n",
      "file 267 completed\n",
      "file 268 completed\n",
      "file 269 completed\n",
      "file 270 completed\n",
      "file 271 completed\n",
      "file 272 completed\n",
      "file 273 completed\n",
      "file 274 completed\n",
      "file 275 completed\n",
      "file 276 completed\n",
      "file 277 completed\n",
      "file 278 completed\n",
      "file 279 completed\n",
      "file 280 completed\n",
      "file 281 completed\n",
      "file 282 completed\n",
      "file 283 completed\n",
      "file 284 completed\n",
      "file 285 completed\n",
      "file 286 completed\n",
      "file 287 completed\n",
      "file 288 completed\n",
      "file 289 completed\n",
      "file 290 completed\n",
      "file 291 completed\n",
      "file 292 completed\n",
      "file 293 completed\n",
      "file 294 completed\n",
      "file 295 completed\n",
      "file 296 completed\n",
      "file 297 completed\n",
      "file 298 completed\n",
      "file 299 completed\n",
      "file 300 completed\n",
      "file 301 completed\n",
      "file 302 completed\n",
      "file 303 completed\n",
      "file 304 completed\n",
      "file 305 completed\n",
      "file 306 completed\n",
      "file 307 completed\n",
      "file 308 completed\n",
      "file 309 completed\n",
      "file 310 completed\n",
      "file 311 completed\n",
      "file 312 completed\n",
      "file 313 completed\n",
      "file 314 completed\n",
      "file 315 completed\n",
      "file 316 completed\n",
      "file 317 completed\n",
      "file 318 completed\n",
      "file 319 completed\n",
      "file 320 completed\n",
      "file 321 completed\n",
      "file 322 completed\n",
      "file 323 completed\n",
      "file 324 completed\n",
      "file 325 completed\n",
      "file 326 completed\n",
      "file 327 completed\n",
      "file 328 completed\n",
      "file 329 completed\n",
      "file 330 completed\n",
      "file 331 completed\n",
      "file 332 completed\n",
      "file 333 completed\n",
      "file 334 completed\n",
      "file 335 completed\n",
      "file 336 completed\n",
      "file 337 completed\n",
      "file 338 completed\n",
      "file 339 completed\n",
      "file 340 completed\n",
      "file 341 completed\n",
      "file 342 completed\n",
      "file 343 completed\n",
      "file 344 completed\n",
      "file 345 completed\n",
      "file 346 completed\n",
      "file 347 completed\n",
      "file 348 completed\n",
      "file 349 completed\n",
      "file 350 completed\n",
      "file 351 completed\n",
      "file 352 completed\n",
      "file 353 completed\n",
      "file 354 completed\n",
      "file 355 completed\n",
      "file 356 completed\n",
      "file 357 completed\n",
      "file 358 completed\n",
      "file 359 completed\n",
      "file 360 completed\n",
      "file 361 completed\n",
      "file 362 completed\n",
      "file 363 completed\n",
      "file 364 completed\n",
      "file 365 completed\n",
      "file 366 completed\n",
      "file 367 completed\n",
      "file 368 completed\n",
      "file 369 completed\n",
      "file 370 completed\n",
      "file 371 completed\n",
      "file 372 completed\n",
      "file 373 completed\n",
      "file 374 completed\n",
      "file 375 completed\n",
      "file 376 completed\n",
      "file 377 completed\n",
      "file 378 completed\n",
      "file 379 completed\n",
      "file 380 completed\n",
      "file 381 completed\n",
      "file 382 completed\n",
      "file 383 completed\n",
      "file 384 completed\n",
      "file 385 completed\n",
      "file 386 completed\n",
      "file 387 completed\n",
      "file 388 completed\n",
      "file 389 completed\n",
      "file 390 completed\n",
      "file 391 completed\n",
      "file 392 completed\n",
      "file 393 completed\n",
      "file 394 completed\n",
      "file 395 completed\n",
      "file 396 completed\n",
      "file 397 completed\n",
      "file 398 completed\n",
      "file 399 completed\n",
      "file 400 completed\n",
      "file 401 completed\n",
      "file 402 completed\n",
      "file 403 completed\n",
      "file 404 completed\n",
      "file 405 completed\n",
      "file 406 completed\n",
      "file 407 completed\n",
      "file 408 completed\n",
      "file 409 completed\n",
      "file 410 completed\n",
      "file 411 completed\n",
      "file 412 completed\n",
      "file 413 completed\n",
      "file 414 completed\n",
      "file 415 completed\n",
      "file 416 completed\n",
      "file 417 completed\n",
      "file 418 completed\n",
      "file 419 completed\n",
      "file 420 completed\n",
      "file 421 completed\n",
      "file 422 completed\n",
      "file 423 completed\n",
      "file 424 completed\n",
      "file 425 completed\n",
      "file 426 completed\n",
      "file 427 completed\n",
      "file 428 completed\n",
      "file 429 completed\n",
      "file 430 completed\n",
      "file 431 completed\n",
      "file 432 completed\n",
      "file 433 completed\n",
      "file 434 completed\n",
      "file 435 completed\n",
      "file 436 completed\n",
      "file 437 completed\n",
      "file 438 completed\n",
      "file 439 completed\n",
      "file 440 completed\n",
      "file 441 completed\n",
      "file 442 completed\n",
      "file 443 completed\n",
      "file 444 completed\n",
      "file 445 completed\n",
      "file 446 completed\n",
      "file 447 completed\n",
      "file 448 completed\n",
      "file 449 completed\n",
      "file 450 completed\n",
      "file 451 completed\n",
      "file 452 completed\n",
      "file 453 completed\n",
      "file 454 completed\n",
      "file 455 completed\n",
      "file 456 completed\n",
      "file 457 completed\n",
      "file 458 completed\n",
      "file 459 completed\n",
      "file 460 completed\n",
      "file 461 completed\n",
      "file 462 completed\n",
      "file 463 completed\n",
      "file 464 completed\n",
      "file 465 completed\n",
      "file 466 completed\n",
      "file 467 completed\n",
      "file 468 completed\n",
      "file 469 completed\n",
      "file 470 completed\n",
      "file 471 completed\n",
      "file 472 completed\n",
      "file 473 completed\n",
      "file 474 completed\n",
      "file 475 completed\n",
      "file 476 completed\n",
      "file 477 completed\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"PLOT:\\s*(.*)\"\n",
    "pos_tags = {}\n",
    "from collections import Counter\n",
    "\n",
    "female_pairs = Counter()\n",
    "male_pairs = Counter()\n",
    "mixed_pairs = Counter()   \n",
    "\n",
    "for file_idx, file_path in enumerate(files):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    match = re.search(pattern, text, re.S)\n",
    "    if not match:\n",
    "        print(f\"file {file_idx} skipped (no PLOT): {file_path}\")\n",
    "        continue\n",
    "\n",
    "    plot = match.group(1).strip()\n",
    "    doc = nlp(plot)\n",
    "    \n",
    "    persons = set()\n",
    "    gpe = set()\n",
    "    names = set()\n",
    "\n",
    "    #we check for entities (PERSON and GPE) in the whole plot\n",
    "    for ent in doc.ents:\n",
    "        if ent.type == \"PERSON\":\n",
    "            pers = ent.text.lower() \n",
    "            names.add(pers) #names of people before concatenation of double names \"snow white\"\n",
    "            gender = get_gender(pers)\n",
    "            concat_pers = \"-\".join(pers.split()) #snow-white\n",
    "            if gender in ['female', 'male']: #to avoid unknowns\n",
    "                if concat_pers not in persons:\n",
    "                    persons.add((concat_pers, ent.type, gender)) \n",
    "                    pos_tags[concat_pers.lower()] = {'pos': ent.type, 'gender': gender}\n",
    "        elif ent.type == \"GPE\":\n",
    "            gpe.add((ent.text, ent.type))\n",
    "\n",
    "    female = [person for person,_,gender in persons if gender=='female']\n",
    "    male = [person for person,_,gender in persons if gender=='male']\n",
    "    \n",
    "    #we go through each sentence and we keep only relevant tokens's lemmas and pos.\n",
    "    for sent in doc.sentences: \n",
    "        sent_tokens = [] #list of relevant tokens to find surrounding words\n",
    "        for token in sent.tokens:\n",
    "            word = token.words[0] #dict with information on the token\n",
    "            text = word.text.lower() #the actual word\n",
    "            pos = word.pos #part-of-speech\n",
    "            lemma = clean_token(word.lemma.lower())\n",
    "            upos = word.upos \n",
    "\n",
    "            if lemma not in stop_words and lemma not in string.punctuation and upos not in [\"PART\", \"PUNCT\", \"NUM\", \"DET\"]:\n",
    "                sent_tokens.append(lemma)\n",
    "                if lemma not in pos_tags:\n",
    "                    pos_tags[lemma]= {'pos': pos, 'gender': None}\n",
    "\n",
    "        sent_tokens = merge_person_tokens(sent_tokens, names) #we merge tokens such as \"snow white\" to \"snow-white\" to match persons.\n",
    "        \n",
    "        tokenized_text, merged_entities = fix_gpe(sent_tokens, gpe) #we merge token such as \"new york city\" to new-your-city\n",
    "        for entity, pos in merged_entities:\n",
    "            if entity not in pos_tags:\n",
    "                pos_tags[entity.lower()] = {'pos': pos, 'gender': None}\n",
    "        \n",
    "        #for each person we find the previous and next 5 words\n",
    "        for (person, _, gender) in persons: \n",
    "            surrounding_words = find_window_words(tokenized_text, person,5) \n",
    "            \n",
    "            gender_to_check = female if gender ==\"male\" else male\n",
    "            same_gender = True\n",
    "            for name in gender_to_check:\n",
    "                if name in surrounding_words:\n",
    "                    same_gender = False\n",
    "\n",
    "            if surrounding_words:\n",
    "                surrounding_words.add(person)\n",
    "            #we create combinatios of all the words in the surrounding window\n",
    "            combs = combinations(surrounding_words, 2)\n",
    "            #if the opposite gender of the person is in the surrounding words, we add the pairs to mixed\n",
    "            for pair in combs:\n",
    "                if gender == \"male\":\n",
    "                    if same_gender:\n",
    "                        male_pairs[tuple(sorted(pair))] += 1\n",
    "                    else:\n",
    "                        mixed_pairs[tuple(sorted(pair))] += 1\n",
    "                elif gender == \"female\":\n",
    "                    if same_gender:\n",
    "                        female_pairs[tuple(sorted(pair))] += 1\n",
    "                    else:\n",
    "                        mixed_pairs[tuple(sorted(pair))] += 1\n",
    "\n",
    "  \n",
    "    print(f'file {file_idx} completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c32afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weighted pairs\n",
    "with open(\"files/male_pairs_w.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for (w1, w2), weight in male_pairs.items():\n",
    "        f.write(f\"{w1},{w2},{weight}\\n\")\n",
    "\n",
    "with open(\"files/female_pairs_w.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for (w1, w2), weight in female_pairs.items():\n",
    "        f.write(f\"{w1},{w2},{weight}\\n\")\n",
    "\n",
    "with open(\"files/mixed_pairs_w.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for (w1, w2), weight in mixed_pairs.items():\n",
    "        f.write(f\"{w1},{w2},{weight}\\n\")\n",
    "\n",
    "# Save POS tags \n",
    "with open(\"files/pos_tags.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(pos_tags, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82119ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\") \n",
    "\n",
    "with open(\"files/pos_tags.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "   pos_tags = json.load(f)\n",
    "   for token in pos_tags.keys():\n",
    "       doc = nlp(token)\n",
    "       if doc.vector_norm:\n",
    "           pos_tags[token]['embedding'] = doc.vector.tolist()\n",
    "       else:\n",
    "           pos_tags[token]['embedding'] = None\n",
    "with open(\"pos_tags.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(pos_tags, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
