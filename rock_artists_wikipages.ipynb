{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ac9a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import json\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab48b4",
   "metadata": {},
   "source": [
    "# Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c32b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of mainstream rock performers\n",
    "# wiki_url = https://en.wikipedia.org/wiki/List_of_mainstream_rock_performers\n",
    "\n",
    "baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "action = \"action=query\"\n",
    "title = \"titles=List_of_mainstream_rock_performers\"\n",
    "content = \"prop=revisions&rvprop=content\"\n",
    "dataformat =\"format=json\"\n",
    "rvslots = \"rvslots=main\"\n",
    "\n",
    "\n",
    "query = \"{}{}&{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat, rvslots)\n",
    "print(query)\n",
    "\n",
    "headers = {\"User-Agent\" : \"MyWikipediaClient/1.0 (example@example.com)\"} # just use this dict as-is.\n",
    "wikirequest = urllib.request.Request(query,None,headers)    # Needed to pass error 403\n",
    "wikiresponse = urllib.request.urlopen(wikirequest)\n",
    "wikidata = wikiresponse.read()\n",
    "wikitext = wikidata.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2902d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON and extract page content\n",
    "wiki_json = json.loads(wikitext)\n",
    "pages = wiki_json['query']['pages']['68324070']['revisions'][0]['slots']['main']['*']\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98af221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of an url listed -- |url=https://www.allmusic.com/artist/10cc-mn0000502163 \n",
    "# Example of a name listed -- \\n* [[10cc]]\n",
    "# Build regex to extract all urls\n",
    "\n",
    "\n",
    "# artist_links = re.findall(r'\\|url=(https?://[^\\s\\|]+)', pages)\n",
    "# artist_links\n",
    "\n",
    "\n",
    "artists = re.findall(r'\\n\\*\\s\\[\\[(.*?)\\]\\]', pages)\n",
    "artists_cleaned = [artist.split('|')[0] for artist in artists]\n",
    "artists_joined = [artist.replace(' ', '_') for artist in artists_cleaned]\n",
    "artists_joined;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccbfd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix = pd.DataFrame(0, index=artists_cleaned, columns=artists_cleaned)\n",
    "word_count_dict = {}\n",
    "\n",
    "\n",
    "for name in tqdm(artists_joined):\n",
    "\n",
    "    # Properly encode the title to handle special characters like Ã–, -, etc.\n",
    "    encoded_title = urllib.parse.quote(name)\n",
    "    artist_title = f\"titles={encoded_title}\"\n",
    "\n",
    "    query = \"{}{}&{}&{}&{}&{}\".format(baseurl, action, content, artist_title, dataformat, rvslots)\n",
    "\n",
    "    try:\n",
    "        artist_wikirequest = urllib.request.Request(query,None,headers)    # Needed to pass error 403\n",
    "        artist_wikiresponse = urllib.request.urlopen(artist_wikirequest)\n",
    "        artist_wikidata = artist_wikiresponse.read()\n",
    "        artist_wikitext = artist_wikidata.decode('utf-8')\n",
    "\n",
    "        artist_wiki_json = json.loads(artist_wikitext)\n",
    "\n",
    "        artist_page_id = list(artist_wiki_json['query']['pages'].keys())[0]\n",
    "        artist_page_content = artist_wiki_json['query']['pages'][artist_page_id]['revisions'][0]['slots']['main']['*']\n",
    "\n",
    "        artist_page_content_valid = re.split(r'==References==', artist_page_content)[0] # split into sections\n",
    "        artist_page_words = re.findall(r'\\w+', artist_page_content_valid)\n",
    "        artist_page_references = re.findall(r\"<ref[^>]*>(.*?)<\\/ref>\", artist_page_content_valid)\n",
    "        artist_page_word_count = len(artist_page_words)-len(artist_page_references) # exclude references from word count\n",
    "        \n",
    "        # Store word count\n",
    "        clean_artist_name = name.replace('_', ' ')\n",
    "        if clean_artist_name not in word_count_dict:\n",
    "            word_count_dict[clean_artist_name] = artist_page_word_count\n",
    "        else:\n",
    "            print(f\"Duplicate entry found for {clean_artist_name}\")\n",
    "\n",
    "        # Check if other artists are mentioned in the output\n",
    "        for artist in artists_cleaned:\n",
    "            if artist in str(artist_page_content) and clean_artist_name != artist: # avoid self loops\n",
    "                adjacency_matrix.loc[clean_artist_name, artist] += 1\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5b8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save adjacency matrix and word counts to CSV files\n",
    "word_count_df = pd.DataFrame(list(word_count_dict.items()), columns=['Artist', 'Word_Count'])\n",
    "word_count_df.to_csv(r'files/rock_artists_word_counts.csv', index=False)\n",
    "adjacency_matrix.to_csv(r'files/rock_artists_adjacency_matrix.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
